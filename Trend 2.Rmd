---
title: "Filtering for momentum strategy"
subtitle: "NYU Tandon MSFE"
author: 
- "Cheng Sun N12763646, cs4530"
- "Jingtao Chen N15309130, jc7229"

output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)

library(Quandl)
Quandl.api_key("DrhA5L6Q1hHNkoo46NiN")
SPX <- Quandl("YAHOO/INDEX_GSPC", type = "xts")$Adj
SPXR <- na.omit(diff(log(SPX)))
SPX_train <- log(SPX["2015-01-01/2016-12-31"])
SPX_test <- log(SPX["2017-01-01/2017-03-05"])
```

<!-- ## Trend -->

<!-- Two very popular momentum trading strategy, together with technical analysis of financial price movement, lies two very important property: -->

<!-- - Trend-following -->
<!-- - Mean-reversion -->

<!-- In this study we focus on the trend-following strategy, and we would like to obtain the trend of a time series. -->


## Trend

"The trend of a time series is considered to be the component containing the global change, which contrasts with local changes due to noise."

$$y_t = x_t + \epsilon_t$$

<!-- Therefore, the trend filtering procedure concerns both noise-canceling and dynamics of the underlying process. -->

A filtering procedure consists of applying a filter $\mathcal L$ to the data $y$:
$$\hat{x} = \mathcal L(y)$$
$y_t$ is the log of price.


## Types of filter

1. Linear filter
    + Moving Average Filter
    + Moving Average Crossover
    + Least Square: L2 filter, Kalman filter

2. Nonlinear filter
    + Nonparametric regression: Loess regression, Spline regression
    + L1 filter
    + Wavelet filter


## Assumption of price

We assume the price follows a lognormal distribution.
$$\frac{d S_t}{S_t} = \mu_t \mathop{d}t + \sigma_t d W_t$$

Then we can define the return as:

$$R_t = \left(\ \mu_t - \frac{1}{2}\sigma_t^2 \right)\Delta + \sigma_t\sqrt{\Delta}\eta_t$$
$\mu_t$ is often regarded as the trend of price returns. 

In this manner, we can easily convert between the trend of log-price and price returns.


## Moving Average Filter

In moving average filter, the filter function is defined as followed:

$$\mathcal L_i = \frac{1}{n}\mathbf{1}\left\{{i < n}\right\}$$

For the return, we have
$$\hat{\mu}_t \simeq \frac{1}{\Delta}\sum_{i=0}^{n-1} \mathcal L_{i}R_{t-i}$$

## Simple moving average
```{r}
knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)
SPX_MA_1 <- na.omit(rollapply(SPX_train, 30, FUN = mean, align = "right"))
SPX_MA_3 <- na.omit(rollapply(SPX_train, 90, FUN = mean, align = "right"))

plot.zoo(merge(merge(SPX_train, SPX_MA_1), SPX_MA_3), plot.type = "single", col = c("black", "red", "blue"),
         xlab = "Year", ylab = "Index(log)", main = "Simple moving average")
legend("bottomleft", c("SPX", "SPX_MA_1", "SPX_MA_3"), col = c("black", "red", "blue"), lty = 1)
```


## Moving Average Crossover

A moving average with window length of *n* is
$$\hat{y}_t^n = \frac{1}{n}\sum_{i=0}^{n-1}y_{t-i}$$

For two moving average with varied window length, the trend of return can be obtained
$$\hat{\mu}_t \simeq \frac{2}{(n_1-n_2)\Delta}\left(\hat{y}_t^{n_2}-\hat{y}_t^{n_1}\right)$$

With the trend of return, the trend of price can be computed as
$$\hat{x}_t = x_{t-1} +\hat{\mu}_t$$

<!-- ## Moving avarage crossover -->

<!-- $n_1 = 90,\,\,\,  n_2=30$ -->

<!-- ```{r} -->
<!-- SPX_MA_1 <- na.omit(rollapply(SPX_train, 30, FUN = mean, align = "right")) -->
<!-- SPX_MA_3 <- na.omit(rollapply(SPX_train, 90, FUN = mean, align = "right")) -->
<!-- mu_hat <- cumprod(1+(SPX_MA_1 - SPX_MA_3)/(90-30))*as.numeric(SPX_train[index(SPX_MA_3)[1]-1]) -->
<!-- plot.zoo(merge(lag(SPX_train), mu_hat), plot.type = "single", col = c("black", "red"), -->
<!--          xlab = "Year", ylab = "Index(log)", main = "Moving average crossover") -->
<!-- legend("bottomleft", c("SPX", "SPX_MA_CO"), col = c("black", "red"), lty = 1) -->
<!-- ``` -->


## L2 filter
We want to solve the function
$$\left\{\hat{x}_1,...,\hat{x}_n\right\}=\arg\min\frac{1}{2}\sum_{t=1}^{n}\left(y_t-\hat{x}_t\right)^2$$

In practice, first we assume the trend of return is a constant. Then we will have the system:
$$x_t = x_{t-1} + \mu\\y_t = \mu t + \epsilon_t$$

Then estimating the trend $\hat{x}_t$ is also equivalent to estimating the coefficient $\mu$
$$\hat{\mu}=\frac{\sum\nolimits_{t=1}^{n}ty_t}{\sum\nolimits_{t=1}^{n}t^2}$$

## L2 filter, cont'd 1
Then if we consider the $\mu$ is not constant, we can define the following objective function, as a tradeoff between errors and smoothness.
$$\frac{1}{2}\sum_{t=1}^{n}\left(y_t-\hat{x}_t\right)^2+\lambda\sum_{t=2}^{n-1}\left(\hat{x}_{t-1}-2\hat{x}_t+\hat{x}_{t+1}\right)^2$$

In vectorial form it is
$$ \frac{1}{2}\left\lVert y-\hat{x} \right\rVert_2^2 + \lambda \left\lVert D\hat{x}\right\rVert_2^2$$

## L2 filter, cont'd 2

where
$$D=\left(
\begin{array}{cc}
1 & -2 & 1 & & & & &\\
  & 1 & -2 & 1\\
 & & & \ddots\\
 &&&&1&-2&1\\
 &&&&&1&2&1
\end{array}\right)$$

Then it can be rewritten as
$$\hat{x}=\left(I+2\lambda D^TD\right)^{-1}y$$

## L2 filter, cont'd 3
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)
# options(warn = 0)
#Load the required packages

#3.L2 filtering
l2filter.optim <- function(x, lambda = 0.0) {
  objective <- function(y, lambda) {
    n <- length(x)
    P1 = 0.5 * sum((y - x)**2)
    P2 = 0
    for (i in 2:(n-1)) {
        P2 = P2 + (y[i-1] - 2 * y[i] + y[i+1])**2
        }
    P1 + lambda * P2
    }
  optim(x, objective, lambda = lambda, method = "BFGS")$par
}

l2filter.matrix <- function(x, lambda = 0.0) {
  n <- length(x)
  
  I = diag(1, nrow = n)
  
  D = matrix(0, nrow = n - 2, ncol = n)
  #
  for (i in 1:(n-2)) {
    D[i, i:(i+2)] = c(1, -2, 1)
  }
  
  xts(c(solve(I + 2 * lambda * t(D) %*% D) %*% coredata(x)), index(x))
}

library(Matrix)

l2filter.sparse <- function(x, lambda = 0.0) {
  n <- length(x)
  
  I = Diagonal(n)
  
  D = bandSparse(n = n - 2, m = n, k = c(0, 1, 2),
                 diagonals = list(rep(1, n), rep(-2, n), rep(1, n)))
  
  xts((solve(I + 2 * lambda * t(D) %*% D) %*% coredata(x))[,1],  index(x))
}

library(zoo)
plot.zoo(merge(SPX_train, l2filter.sparse(SPX_train, 1)), plot.type = "single", col = c("black", "red"), 
         xlab = "Year", ylab = "Index(log)", main = "L2 filtering")
legend("bottomleft", c("SPX", "SPX_L2"), col = c("black", "red"), lty = 1)
```


## Kalman filter

The dynamic of the system can be expressed as
$$\begin{cases}R_t=\mu_t+\sigma_{\zeta}\zeta_t\\\mu_t=\mu_{t-1}+\sigma_{\eta}\eta_t\end{cases}$$

The estimation of $\mu_t$ can be defined as
$$\hat{\mu}_{t|t-1}=\mathbb{E}_{t-1}\left[\mu_t\right]$$

The variance of $\mu_t$, $P_t$ is defined as
$$P_{t|t-1}=\mathbb{E}_{t-1}\left[\left(\hat{\mu}_{t|t-1}-\mu_t\right)^2\right]$$

## Kalman filter, cont'd 1

Therefore we can define that
$$\hat{\mu}_{t+1|t}=(1-K_t)\hat{\mu}_{t|t-1}+K_tR_t$$
in which
$$K_t=\frac{P_{t|t-1}}{P_{t|t-1}+\sigma_{\zeta}^2}$$

In practice, one method of solving the equation is
$$\hat{\mu}_{t+1|t}=(1-\kappa)\hat{\mu}_{t|t-1}+\kappa R_t$$

where$\kappa=\frac{2\sigma_{\eta}}{\sigma_{\eta}+\sqrt{\sigma_{\eta}^2+4\sigma_{\zeta}^2}}$

## Kalman filter, cont'd 2
If we wrote the formula as this, we can have $\lambda =-\ln(1-\kappa)$

Then the prediction of this single time series can be written as
$$\hat{\mu}_t =\left(1-e^{-\lambda}\right)\sum_{i=0}^{\infty} e^{-\lambda i} R_{t-i}$$
And the trend of log-price is
$$\hat{x}_t = \left(1-e^{-\lambda}\right)\sum_{i=0}^{\infty} e^{-\lambda i}y_{t-i}$$

## Kalman Filter, cont'd 3
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)

kmfilter <- function(x, lambda = 0.0) {
  n <- length(x)
  xhat <- numeric(n)
  for(i in 1:n){
    xhat[i] <- (1 - exp(-lambda))*sum(exp(-lambda*((i-1):0))*x[1:i])
  }
  xts(xhat, index(x))[-(1:10)]
}
plot.zoo(merge(SPX_train, kmfilter(SPX_train, 0.5)[-(1:5)]), plot.type = "single", col = c("black", "red"), 
         xlab = "Year", ylab = "Index(log)", main = "Kalman filtering")
legend("bottomleft", c("SPX", "SPX_KM"), col = c("black", "red"), lty = 1)
```


## Loess regression
For the local polynomial regression
$$\begin{align}y_t & \,=\, f(t) \,+\, \epsilon_t \\& \,=\, \beta_0(\tau) \,+\, \sum_{j=1}^{p}\beta_j(\tau)(\tau \, - t)^j \,+\,\epsilon_t\ \\\end{align}$$

we use it to get the residuals $\hat{\epsilon_t}$, then we compute $\delta_t \,=\, (1 - \mu_t^2) \cdot 1\{|\mu_t|\leq 1\}$ with $\mu_t = \hat{\epsilon}_t / (6\, \text{median}|\hat{\epsilon}|)$ 
and run a second kernel regression with weightings $\delta_t w_t$

## Loess regression, cont'd 1
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)

suppressPackageStartupMessages(library(ggvis))
options(warn = 0)
library(ggvis)

y = coredata(SPX_train)
colnames(y) = "y"
x = index(SPX_train)
da <- data.frame(cbind(y,x))
lw = loess(y~x, da, span = 0.5)
plot.zoo(merge(SPX_train, lw$fitted), plot.type = "single", col = c("black", "red"), 
         xlab = "Year", ylab = "Index(log)", main = "Loess regression")
legend("bottomleft", c("SPX", "SPX_LR"), col = c("black", "red"), lty = 1)

da %>% ggvis(~x, ~y) %>%
  layer_lines() %>%
  layer_smooths(span = input_slider(label = "Choose lambda:",min = 0.1, max = 1),stroke:= "red") %>%
  add_axis("x", title = "Year") %>%
  add_axis("y", title = "Index(log)")
```


## L1 filter
Similar to the L2 filter, and much like the LASSO regression, we now consider the L1 term to be a smoothness penalty function.
The objective function can then be written as
$$\frac{1}{2} \sum_{t=1}^{n} \left(y_t - \hat{x}_t\right)^2 \,+\,\lambda \sum_{t=2}^{n-1}\left|\hat{x}_{t-1}-2\hat{x}_t+\hat{x}_{t+1}\right|$$

The equivalent vectorial form is
$$\frac12 \left\lVert y-\hat{x} \right\rVert_2^2 \,+\,\lambda \left\lVert D\hat{x} \right\rVert_1  $$

## L1 filter, cont'd
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)
# options(warn = 0)
l1filter.optim <- function (x, lambda = 0.0) {
  objective <- function(y, lambda) {
    n <- length(x)
    
    P1 = 0.5 * sum((y - coredata(x))**2)
    #
    P2 = 0
    for (i in 2:(n-1)) {
      P2 = P2 + abs(y[i-1] - 2 * y[i] + y[i+1])
    }
    #
    P1 + lambda * P2
  }
  #
  fit = optim(x, objective, lambda = lambda, method = "CG", control = list(maxit = 100000, type = 3))
  
  if (fit$convergence != 0) {
    warning(sprintf("Optimisation failed to converge! (lambda = %f)", lambda))
    print(fit)
  }
  
  return(fit$par)
}
l1 <- l1filter.optim(SPX_train, 0.5)
plot.zoo(merge(SPX_train, l1), plot.type = "single", col = c("black", "red"), 
         xlab = "Year", ylab = "Index(log)", main = "L1 filtering")
legend("bottomleft", c("SPX", "SPX_L1"), col = c("black", "red"), lty = 1)
```


## Wavelet filter
The fourier transformation allows us to consider original signal as a combination of frequency functions
$$y(\omega)=\sum_{t=1}^n y_t e^{-iwt} $$

To avoid difficulties when trend reverses, we will have Wavelet filter, which takes into consideration both frequency and time.

## Wavelet filter, cont'd 1

1. First we compute the wavelet transformation $\mathcal W$ of $y_t$ and get wavelet coefficients $\omega = \mathcal W(y)$

2. We apply a denoising rule $D$:
$$\omega^* = D(\omega)$$

3. Use the inverse Wavelet transformation $\mathcal W^{-1}$ we get
$$x=\mathcal W^{-1}\left(\omega^*\right)$$


## Wavelet filter, cont'd 2
The shrinkage methods includes:
Let $\omega^-$, $\omega^+$ be two scalars with $0<\omega^-<\omega^+$


* Hard Shrinkage

$$\omega_i^* = \omega_i \cdot \mathbb{1}\left\{ \left|\omega_i\right| > \omega^+ \right\}$$

* Soft Shrinkage

$$\omega_i^* = \text{sgn}(\omega_i) \cdot \left( \left|\omega_i\right| - \omega^+  \right)_+ $$

* Semi-soft shrinkage

$$ \omega_i^* = 
\begin{cases}
0 &  \, |\omega_i| \leq \omega^-\\
\text{sgn}(\omega_i)(\omega^+-\omega^-)^{-1}\omega^+(|\omega_i|-\omega^-) &  \,\, \omega^-<|\omega_i| \leq \omega^+ \\
\omega_i &  \, |\omega_i| > \omega^+
\end{cases}$$


* Quantile shrinkage is a hard shrinkage method where $\omega^+$ is the $q^{th}$ quantile of the coefficients $|\omega_i|$

## Wavelet filter, cont'd 3
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)
# suppressPackageStartupMessages(library(wavethresh))
library(wavethresh)
options(warn = 0)

 Time <- 1:length(SPX_train)
 Accel <- coredata(SPX_train)
#
# Rescale Time to [0,1]

 Time01 <- (Time - min(Time))/(max(Time) - min(Time))
#
# Interpolate data to grid
#
 McycleGrid <- makegrid(t=Time01, y=Accel)
#
# Scale new [0,1] grid back to original scale
#
 TimeGrid<-McycleGrid$gridt*(max(Time)-min(Time))+min(Time)
#
# Plot interpolated data
#
 # lines(TimeGrid, McycleGrid$gridy)
# Perform KS00 irregular wavelet transform
#
 McycleIRRWD <- irregwd(McycleGrid)
#
# Convert the irregwd object to wd for coef plotting
#
 McycleIRRWD2 <- McycleIRRWD
 class(McycleIRRWD2) <- "wd"
 # plot(McycleIRRWD2)
# Do thresholding
#
 McycleT <- threshold(McycleIRRWD, policy="universal",type="soft", dev=madmad)
#
# Invert and plot, and original
#
 McycleWR <- wr(McycleT)
 plot.zoo(merge(SPX_train, McycleWR[-(1:8)]), plot.type = "single", col = c("black", "red"), 
          xlab = "Year", ylab = "Index(log)", main = "Wavelet filtering")
 legend("bottomleft", c("SPX", "SPX_WL"), col = c("black", "red"), lty = 1)
```

## Trend detection

To apply filters to data we need to see if trend does exist.

We use Mann's (1945) statistics to detect trend:
$$\mathbb S_t^{(n)} = \sum_{i=0}^{n-2} \sum_{j=i+1}^{n-1} \text{sgn} (y_{t-i} - y_{t-j}) $$
$$-\frac{n(n+1)}{2} \leq \mathbb S_t^{(n)} \leq \frac{n(n+1)}{2} $$
If there is no trend, $\mathbb S_t^{(n)} \simeq 0 $, then we can test $Z_t^{(n)}$
$$Z_t^{(n)} = \frac{\mathbb S_t^{(n)}}{\sqrt{\text{var} \left(\mathbb S_t^{(n)} \right)}} $$

## Trend detection, cont'd 1
The paper compiled a table of significance to reject the $\mathbb S_t^{(n)} = 0$ hypothesis based on certain $\alpha$ value and window length.

------------------------------------
$\alpha$        90%    95%     99%
------------- ------ ------- -------
  n=10 days   58.06% 49.47%  29.37%
  
  n=3 months  85.77% 82.87%  76.68%
  
  n=1 year    97.17% 96.78%  95.33%
------------------------------------

## Prediction results

The paper studied returns across indxes for one-month return following a 3 month period of positive/negative trend.

-------------------------------------------
Trend       Positive  Negative  Difference  
----------- --------- --------- -----------
Eurostoxx50    1.1%      0.2%      0.9%

S&P500         0.9%      0.5%      0.4%

MSCI WORLD     0.6%     -0.3%      1.0%

MSCI EM        1.9%     -0.3%      2.2%

TOPIX          0.4%     -0.4%      0.9%

EUR/USD        0.2%     -0.2%      0.4%

USD/JPY        0.2%     -0.2%      0.4%

GSCI           1.3%     -0.4%      1.6%

-------------------------------------------

It clearly states the usefulness of trend strategies. Once we can obtain the trend using our strategies metioned above we can hopefully capture profits.

<!-- ## Trading based on moving average crossover -->


<!-- ```{r results="hide"} -->
<!-- knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE, results = "hide") -->
<!-- library(quantmod) -->
<!-- library(PerformanceAnalytics) -->
<!-- require(RCurl) -->
<!-- sit = getURLContent('https://github.com/systematicinvestor/SIT/raw/master/sit.gz', binary=TRUE, followlocation = TRUE, ssl.verifypeer = FALSE) -->
<!-- con = gzcon(rawConnection(sit, 'rb')) -->
<!-- source(con) -->
<!-- close(con) -->
<!-- data <- new.env() -->

<!-- # Load historical data and adjusts for splits and dividends -->
<!-- tickers = spl('SPY') -->
<!-- getSymbols(tickers, src = 'yahoo', from = '2000-01-01', env = data, auto.assign = T)     -->
<!-- for(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)   -->

<!-- #Calculate the moving averages and lag them one day to prevent lookback bias -->
<!-- PreviousSMA_50 <- lag(SMA(Cl(data[['SPY']]),50))   -->
<!-- PreviousSMA_200 <- lag(SMA(Cl(data[['SPY']]),200)) -->

<!-- #Sets backtesting environment -->
<!-- bt.prep(data, align='remove.na')  -->
<!-- prices = data$prices    -->

<!-- #Create a empty list for attaching the models to at a later stage -->
<!-- models = list() -->

<!-- #Specify the weights to be used in the backtest -->
<!-- data$weight[] = NA #Zero out any weights from previous -->
<!-- data$weight[] = ifelse(as.integer(PreviousSMA_50>PreviousSMA_200)==1,1,-1)  #If price of SPY is above the SMA then buy -->

<!-- #Call the function to run the backtest given the data, which contains the prices and weights. -->
<!-- models$technical_model = bt.run.share(data, trade.summary=T) -->

<!-- #Plot equity curve and export the trades list to csv -->
<!-- plot(models$technical_model$equity, main="Equity Curve") -->

<!-- # Returns -->
<!-- dailyBH<-models$technical_model$equity/na.omit(lag.xts(models$technical_model$equity))-1 -->

<!-- # Return Statistics  -->
<!-- annualizedReturn <- Return.annualized(dailyBH,scale=252, geometric=FALSE) -->
<!-- cumulativeReturn <-Return.cumulative(dailyBH) -->

<!-- # Risk Statistics -->
<!-- annualizedSTD <-sd.annualized(dailyBH,scale=252) -->
<!-- maxDrawdown <- maxDrawdown(dailyBH) -->

<!-- # Sharpe Ratio -->
<!-- excessBH <- dailyBH[2:length(dailyBH)]-.02/252 -->
<!-- dailyBHstd <- sd(dailyBH) -->
<!-- avgExcessBH <- mean(excessBH) -->

<!-- sharpeBH <- 10*(avgExcessBH/dailyBHstd*sqrt(252)) -->
<!-- ``` -->

## Refrences
[1] TREND FILTERING METHODS FOR MOMENTUM STRATEGIES, Benjamin Bruder et al, Lyxor Asset Management

[2] Understanding Extended Kalman Filter ¨C Part II: Multi-dimensional Kalman Filter, Shudong Huang, University of Technology Sydney

[3] Wavelet Methods in Statistics with R, Guy Nason
