---
title: "Filtering for momentum strategy"
subtitle: "NYU Tandon MSFE"
author: 
- "Jingtao Chen"

output: ioslides_presentation
---

```{r setup, include=FALSE, echo=FALSE}

library(Quandl)
Quandl.api_key("DrhA5L6Q1hHNkoo46NiN")
SPX <- log(Quandl("YAHOO/INDEX_GSPC", type = "xts", start_date="2015-01-01", end_date ="2017-03-05")$Adj)
SPXR <- na.omit(diff(log(SPX)))
SPX_train <- SPX["2015-01-01/2016-12-31"]
SPX_test <- SPX["2017-01-01/2017-03-05"]

length(SPX_train)
length(SPX)
```

<!-- ## Trend -->

<!-- Two very popular momentum trading strategy, together with technical analysis of financial price movement, lies two very important property: -->

<!-- - Trend-following -->
<!-- - Mean-reversion -->

<!-- In this study we focus on the trend-following strategy, and we would like to obtain the trend of a time series. -->


## Trend

"The trend of a time series is considered to be the component containing the global change, which contrasts with local changes due to noise."

$$y_t = x_t + \epsilon_t$$

<!-- Therefore, the trend filtering procedure concerns both noise-canceling and dynamics of the underlying process. -->

A filtering procedure consists of applying a filter $\mathcal L$ to the data $y$:
$$\hat{x} = \mathcal L(y)$$


## Types of filter

1. Linear filter
    + Moving Average Filter
    + Moving Average Crossover
    + Least Square: L2 filter, Kalman filter

2. Nonlinear filter
    + Loess regression
    + L1 filter
    + Wavelet filter


## Assumption of price

We assume the price follows a lognormal distribution.
$$\frac{d S_t}{S_t} = \mu_t \mathop{d}t + \sigma_t d W_t$$

Then we can define the return as:

$$R_t = \left(\ \mu_t - \frac{1}{2}\sigma_t^2 \right)\Delta + \sigma_t\sqrt{\Delta}\eta_t$$
$\mu_t$ is often regarded as the trend of price returns. 

In this manner, we can easily convert between the trend of log-price and price returns.


## Moving Average Filter

In moving average filter, the filter function is defined as followed:

$$\mathcal L_i = \frac{1}{n}\mathbf{1}\left\{{i < n}\right\}$$

For the return, we have
$$\hat{\mu}_t \simeq \frac{1}{\Delta}\sum_{i=0}^{n-1} \mathcal L_{i}R_{t-i}$$

## Simple moving average
```{r results="hide", echo=FALSE}
SPX_MA_1 <- na.omit(rollapply(SPX_train, 30, FUN = mean, align = "right"))
SPX_MA_3 <- na.omit(rollapply(SPX_train, 90, FUN = mean, align = "right"))

plot.zoo(merge(merge(SPX_train, SPX_MA_1), SPX_MA_3), col = c("black", "red", "blue"), 
         xlab = "Year", ylab = "Index(log)", main = "Simple moving average")
legend("bottomleft", c("SPX", "SPX_MA_1", "SPX_MA_3"), col = c("black", "red", "blue"), lty = 1)
```


## L2 filter
We want to solve the function
$$\left\{\hat{x}_1,...,\hat{x}_n\right\}=\arg\min\frac{1}{2}\sum_{t=1}^{n}\left(y_t-\hat{x}_t\right)^2$$

In practice, first we assume the trend of return is a constant. Then we will have the system:
$$x_t = x_{t-1} + \mu\\y_t = \mu t + \epsilon_t$$

Then estimating the trend $\hat{x}_t$ is also equivalent to estimating the coefficient $\mu$
$$\hat{\mu}=\frac{\sum\nolimits_{t=1}^{n}ty_t}{\sum\nolimits_{t=1}^{n}t^2}$$

## L2 filter, cont'd 1
Then if we consider the $\mu$ is not constant, we can define the following objective function, as a tradeoff between errors and smoothness.
$$\frac{1}{2}\sum_{t=1}^{n}\left(y_t-\hat{x}_t\right)^2+\lambda\sum_{t=2}^{n-1}\left(\hat{x}_{t-1}-2\hat{x}_t+\hat{x}_{t+1}\right)^2$$

In vectorial form it is
$$ \frac{1}{2}\left\lVert y-\hat{x} \right\rVert_2^2 + \lambda \left\lVert D\hat{x}\right\rVert_2^2$$

## L2 filter, cont'd 2

where
$$D=\left(
\begin{array}{cc}
1 & -2 & 1 & & & & &\\
  & 1 & -2 & 1\\
 & & & \ddots\\
 &&&&1&-2&1\\
 &&&&&1&2&1
\end{array}\right)$$

Then it can be rewritten as
$$\hat{x}=\left(I+2\lambda D^TD\right)^{-1}y$$

## L2 filter, cont'd 3
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
l2filter.optim <- function(x, lambda = 0.0) {
  objective <- function(y, lambda) {
    n <- length(x)
    P1 = 0.5 * sum((y - x)**2)
    P2 = 0
    for (i in 2:(n-1)) {
        P2 = P2 + (y[i-1] - 2 * y[i] + y[i+1])**2
        }
    P1 + lambda * P2
    }
  optim(x, objective, lambda = lambda, method = "BFGS")$par
}

l2filter.matrix <- function(x, lambda = 0.0) {
  n <- length(x)
  
  I = diag(1, nrow = n)
  
  D = matrix(0, nrow = n - 2, ncol = n)
  #
  for (i in 1:(n-2)) {
    D[i, i:(i+2)] = c(1, -2, 1)
  }
  
  xts(c(solve(I + 2 * lambda * t(D) %*% D) %*% coredata(x)), index(x))
}

library(Matrix)

l2filter.sparse <- function(x, lambda = 0.0) {
  n <- length(x)
  
  I = Diagonal(n)
  
  D = bandSparse(n = n - 2, m = n, k = c(0, 1, 2),
                 diagonals = list(rep(1, n), rep(-2, n), rep(1, n)))
  
  xts((solve(I + 2 * lambda * t(D) %*% D) %*% coredata(x))[,1],  index(x))
}

library(zoo)
plot.zoo(merge(merge(SPX_train, l2filter.sparse(SPX_train, 1)),l2filter.sparse(SPX_train, 100)), col = c("black", "red", "blue"), 
         xlab = "Year", ylab = "Index(log)", main = "L2 filtering")
legend("bottomright", c("SPX", "SPX_L2_1", "SPX_L2_100"), col = c("black", "red", "blue"), lty = 1, cex = 0.75)
```


## Kalman filter

The dynamic of the system can be expressed as
$$\begin{cases}R_t=\mu_t+\sigma_{\zeta}\zeta_t\\\mu_t=\mu_{t-1}+\sigma_{\eta}\eta_t\end{cases}$$

The estimation of $\mu_t$ can be defined as
$$\hat{\mu}_{t|t-1}=\mathbb{E}_{t-1}\left[\mu_t\right]$$

The variance of $\mu_t$, $P_t$ is defined as
$$P_{t|t-1}=\mathbb{E}_{t-1}\left[\left(\hat{\mu}_{t|t-1}-\mu_t\right)^2\right]$$

## Kalman filter, cont'd 1

Therefore we can define that
$$\hat{\mu}_{t+1|t}=(1-K_t)\hat{\mu}_{t|t-1}+K_tR_t$$
in which
$$K_t=\frac{P_{t|t-1}}{P_{t|t-1}+\sigma_{\zeta}^2}$$

In practice, one method of solving the equation is
$$\hat{\mu}_{t+1|t}=(1-\kappa)\hat{\mu}_{t|t-1}+\kappa R_t$$

where$\kappa=\frac{2\sigma_{\eta}}{\sigma_{\eta}+\sqrt{\sigma_{\eta}^2+4\sigma_{\zeta}^2}}$

## Kalman filter, cont'd 2
If we wrote the formula as this, we can have $\lambda =-\ln(1-\kappa)$

Then the prediction of this single time series can be written as
$$\hat{\mu}_t =\left(1-e^{-\lambda}\right)\sum_{i=0}^{\infty} e^{-\lambda i} R_{t-i}$$
And the trend of log-price is
$$\hat{x}_t = \left(1-e^{-\lambda}\right)\sum_{i=0}^{\infty} e^{-\lambda i}y_{t-i}$$

## Kalman Filter, cont'd 3
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
kmfilter <- function(x, lambda = 0.0) {
  n <- length(x)
  xhat <- numeric(n)
  for(i in 1:n){
    xhat[i] <- (1 - exp(-lambda))*sum(exp(-lambda*((i-1):0))*x[1:i])
  }
  xts(xhat, index(x))[-(1:10)]
}

plot.zoo(merge(merge(SPX_train, kmfilter(SPX_train, 0.7)[-(1:5)]),kmfilter(SPX_train, 0.3)[-(1:5)]), col = c("black", "red", "blue"), 
         xlab = "Year", ylab = "Index(log)", main = "Kalman filtering")
legend("bottomright", c("SPX", "SPX_KM_0.7", "SPX_KM_0.3"), col = c("black", "red", "blue"), lty = 1, cex = 0.75)
```


## Loess regression
For the local polynomial regression
$$\begin{align}y_t & \,=\, f(t) \,+\, \epsilon_t \\& \,=\, \beta_0(\tau) \,+\, \sum_{j=1}^{p}\beta_j(\tau)(\tau \, - t)^j \,+\,\epsilon_t\ \\\end{align}$$

we use it to get the residuals $\hat{\epsilon_t}$, then we compute $\delta_t \,=\, (1 - \mu_t^2) \cdot 1\{|\mu_t|\leq 1\}$ with $\mu_t = \hat{\epsilon}_t / (6\, \text{median}|\hat{\epsilon}|)$ 
and run a second kernel regression with weightings $\delta_t w_t$

## Loess regression, cont'd 1
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
y = coredata(SPX_train)
colnames(y) = "y"
x = 1:length(SPX_train)
da <- data.frame(cbind(y,x))
lw1 = loess(y~x, da, span = 0.7)
predict(lw1, 511)
lw2 = loess(y~x, da, span = 0.3)

plot.zoo(merge(merge(SPX_train, lw1$fitted),lw2$fitted), col = c("black", "red", "blue"), 
         xlab = "Year", ylab = "Index(log)", main = "Loess regression")
legend("bottomright", c("SPX", "SPX_LR_0.7", "SPX_LR_0.3"), col = c("black", "red", "blue"), lty = 1, cex = 0.75)
```


## L1 filter
Similar to the L2 filter, and much like the LASSO regression, we now consider the L1 term to be a smoothness penalty function.
The objective function can then be written as
$$\frac{1}{2} \sum_{t=1}^{n} \left(y_t - \hat{x}_t\right)^2 \,+\,\lambda \sum_{t=2}^{n-1}\left|\hat{x}_{t-1}-2\hat{x}_t+\hat{x}_{t+1}\right|$$

The equivalent vectorial form is
$$\frac12 \left\lVert y-\hat{x} \right\rVert_2^2 \,+\,\lambda \left\lVert D\hat{x} \right\rVert_1  $$

## L1 filter, cont'd
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)
# options(warn = 0)
l1filter.optim <- function (x, lambda = 0.0) {
  objective <- function(y, lambda) {
    n <- length(x)
    
    P1 = 0.5 * sum((y - coredata(x))**2)
    #
    P2 = 0
    for (i in 2:(n-1)) {
      P2 = P2 + abs(y[i-1] - 2 * y[i] + y[i+1])
    }
    #
    P1 + lambda * P2
  }
  #
  fit = optim(x, objective, lambda = lambda, method = "CG", control = list(maxit = 100000, type = 3))
  
  if (fit$convergence != 0) {
    warning(sprintf("Optimisation failed to converge! (lambda = %f)", lambda))
    print(fit)
  }
  
  return(fit$par)
}
l1 <- l1filter.optim(SPX_train, 0.5)

plot.zoo(merge(SPX_train, l1), col = c("black", "red"), 
         xlab = "Year", ylab = "Index(log)", main = "L1 filtering")
legend("bottomright", c("SPX", "SPX_L1"), col = c("black", "red"), lty = 1, cex = 0.75)
```


## Wavelet filter
The fourier transformation allows us to consider original signal as a combination of frequency functions
$$y(\omega)=\sum_{t=1}^n y_t e^{-iwt} $$

To avoid difficulties when trend reverses, we will have Wavelet filter, which takes into consideration both frequency and time.

## Wavelet filter, cont'd 1

1. First we compute the wavelet transformation $\mathcal W$ of $y_t$ and get wavelet coefficients $\omega = \mathcal W(y)$

2. We apply a denoising rule $D$:
$$\omega^* = D(\omega)$$

3. Use the inverse Wavelet transformation $\mathcal W^{-1}$ we get
$$x=\mathcal W^{-1}\left(\omega^*\right)$$


## Wavelet filter, cont'd 2
The shrinkage methods includes:
Let $\omega^-$, $\omega^+$ be two scalars with $0<\omega^-<\omega^+$


* Hard Shrinkage

$$\omega_i^* = \omega_i \cdot \mathbb{1}\left\{ \left|\omega_i\right| > \omega^+ \right\}$$

* Soft Shrinkage

$$\omega_i^* = \text{sgn}(\omega_i) \cdot \left( \left|\omega_i\right| - \omega^+  \right)_+ $$

* Semi-soft shrinkage

$$ \omega_i^* = 
\begin{cases}
0 &  \, |\omega_i| \leq \omega^-\\
\text{sgn}(\omega_i)(\omega^+-\omega^-)^{-1}\omega^+(|\omega_i|-\omega^-) &  \,\, \omega^-<|\omega_i| \leq \omega^+ \\
\omega_i &  \, |\omega_i| > \omega^+
\end{cases}$$


* Quantile shrinkage is a hard shrinkage method where $\omega^+$ is the $q^{th}$ quantile of the coefficients $|\omega_i|$

## Wavelet filter, cont'd 3
```{r results="hide", warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
# knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE)
# suppressPackageStartupMessages(library(wavethresh))
library(wavethresh)
options(warn = 0)

 Time <- 1:length(SPX_train)
 Accel <- coredata(SPX_train)
#
# Rescale Time to [0,1]

 Time01 <- (Time - min(Time))/(max(Time) - min(Time))
#
# Interpolate data to grid
#
 McycleGrid <- makegrid(t=Time01, y=Accel)
#
# Scale new [0,1] grid back to original scale
#
 TimeGrid<-McycleGrid$gridt*(max(Time)-min(Time))+min(Time)
#
# Plot interpolated data
#
 # lines(TimeGrid, McycleGrid$gridy)
# Perform KS00 irregular wavelet transform
#
 McycleIRRWD <- irregwd(McycleGrid)
#
# Convert the irregwd object to wd for coef plotting
#
 McycleIRRWD2 <- McycleIRRWD
 class(McycleIRRWD2) <- "wd"
 # plot(McycleIRRWD2)
# Do thresholding
#
 McycleT <- threshold(McycleIRRWD, policy="universal",type="soft", dev=madmad)
#
# Invert and plot, and original
#
 McycleT1 <- threshold(McycleIRRWD, policy="universal",type="soft", dev=madmad)
 McycleT2 <- threshold(McycleIRRWD, policy="universal",type="hard", dev=madmad)
#
# Invert and plot, and original
#
 McycleWR1 <- wr(McycleT1)
 McycleWR2 <- wr(McycleT2)
 plot.zoo(merge(merge(SPX_train, McycleWR1[-(1:8)]),McycleWR2[-(1:8)]), col = c("black", "red", "blue"), 
          xlab = "Year", ylab = "Index(log)", main = "Wavelet filtering")
 legend("bottomright", c("SPX", "SPX_WL_SOFT", "SPX_WL_HARD"), col = c("black", "red", "blue"), lty = 1, cex = 0.75)
```


## Get the best parameter: Moving Average
The cross-validation gives little hint about moving average, since the shorter-term moving average almost surely closer to the actual data and produces less errors.
```{r, results="hide", echo=FALSE}
M1_1 <- sapply(c(30, 90), function(x){
  series <- sapply(504:545, function(y){
    ts <- na.omit(rollapply(SPX[1:y], x, FUN = mean, aligh = "right"))
    2*last(ts) - last(ts, 2)
  })
  sum((SPX_test - series)^2) /41
})
plot(M1_1, type = "l")
which.min(M1_1) #n = 30
```


## Get the best parameter: L2

For L2 term we try to get the best lambda. The best lambda is therefore 0.06.

```{r results="hide", echo=FALSE}
library(Matrix)

l2filter.sparse <- function(x, lambda = 0.0) {
  n <- length(x)
  
  I = Diagonal(n)
  
  D = bandSparse(n = n - 2, m = n, k = c(0, 1, 2),
                 diagonals = list(rep(1, n), rep(-2, n), rep(1, n)))
  
  xts((solve(I + 2 * lambda * t(D) %*% D) %*% coredata(x))[,1],  index(x))
}
# M2 <- sapply(10^(-2:5), function(x){ sum((SPX_test - 
#                                   sapply(504:545, function(y) last(l2filter.sparse(SPX[1:y], lambda = x))))^2)/41
# })
# plot(M2, type = "l", xaxt = "n")
# axis(1, at = 1:8, labels = 10^(-2:5))
# which.min(M2)
M2 <- sapply(seq(0.01,1,0.01), function(x){
  series <- sapply(504:545, function(y){
    ts <- l2filter.sparse(SPX[1:y], lambda = x)
    last(ts)*2 - last(ts, 2)
  })
  sum((SPX_test - series)^2)/41
})

# M2_1 <- sapply(seq(0.01,1,0.01), function(x){ sum((SPX_test - 
#                                             sapply(504:545, function(y) last(l2filter.sparse(SPX[1:y], lambda = x))))^2)/41
# })
plot(M2, type = "l", xaxt = "n")
axis(1, at = seq(0, 100, 20), labels = seq(0, 100, 20)/100)
which.min(M2) #lambda = 0.06
```


## Get the best parameter: Kalman filter

With Kalman filter we would like to get the best lambda through time series cross validation as well. In this case the optimal lambda value is 0.12.

```{r results="hide", echo=FALSE}
kmfilter <- function(x, lambda = 0.0) {
  n <- length(x)
  xhat <- numeric(n)
  for(i in 1:n){
    xhat[i] <- (1 - exp(-lambda))*sum(exp(-lambda*((i-1):0))*x[1:i])
  }
  xts(xhat, index(x))[-(1:10)]
}
# M3 <- sapply(10^(-2:5), function(x){ sum((SPX_test - 
#                                   sapply(504:545, function(y) last(kmfilter(SPX[1:y], lambda = x))))^2)/41
# })
M3 <- sapply(seq(0.01, 1, length.out = 10), function(x){
  series <- sapply(504:545, function(y){
    tss <- kmfilter(SPX[1:y], lambda = x)
    coredata(last(tss)) * (2 - exp(-x)) - sum((1-exp(-x))*(exp(x)-1)*exp(-x * 1:y) * coredata(SPX[1:y]))
  })
  sum((SPX_test - series)^2)/41
})
plot(M3, type = "l", xaxt = "n")
axis(1, at = 1:10, labels = seq(0.01, 1, length.out = 10))
which.min(M3) # lambda 0.12

# M3_1 <- sapply(1:10, function(x){ sum((SPX_test - 
#                                             sapply(504:545, function(y) last(kmfilter(SPX[1:y], lambda = x))))^2)/41
# })
# plot(M3_1, type = "l")
# which.min(M3_1) #lambda = 3
```


## Get the best parameter: Loess regression

In loess regression, the lambda controls degree of freedom for local polynomials. The lambda is set to 0.02.

```{r results="hide", echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
loessown <- function(x, lambda = 0.0){
  da <- data.frame(cbind(coredata(x),1:length(x)))
  names(da) <- c("y", "x")
  last(loess(y~x, da, span = lambda)$fitted)
}

M4_1 <- sapply(c(seq(0.01, 1, by = 0.01)), function(x){ sum((SPX_test - sapply(504:545, function(y) last(loessown(SPX[1:y], lambda = x))))^2)/41})

plot(M4_1, type = "l", xaxt = "n")
axis(1, at = 1:100, labels = seq(0.01, 1, by=0.01))
which.min(M4_1) # lambda = 0.02
```

## Get the best parameter: L1 filter

For the L1 filter, it we set the error term to minimum, clearly lambda term should be minimum. In the following example, we choose 0.001 as the optimal lambda.

```{r results="hide", echo=FALSE}
library(l1tf)

l1fs <- function(x, lambda = 0.0){
  da <- x
  fitted <- l1tf(da, lambda = lambda)
  last(fitted)*2 - last(fitted, 2)
}

Ml1 <- sapply(10^(-3:3), function(x){ sum((SPX_test -
                                                 sapply(504:545, function(y) last(l1fs(SPX[1:y], lambda = x))))^2)/41
})
# Ml1 <- sapply(10^(-3:3), function(x){
#   pred <- sapply(504:545, function(y){l1fs(SPX[1:y], lambda = x)})
#   sum((SPX_test - pred)^2)
# })

plot(Ml1, type = "l", xaxt = "n")
axis(1, at = 1:7, labels = 10^(-3:3))
which.min(Ml1) # lambda = 0.001
```


## Get the best parameter: Wavelet

For Wavelet transformation, we want to make sure which kind of shrinkage methods yields the best trends for prediction.

Based on the findings below, we choose the "soft" threshold.

```{r results="hide", echo=FALSE}
library(wavethresh)

waveletown <- function(x, rule = "soft"){
  Time <- 1:length(x)
  Accel <- coredata(x)
  Time01 <- (Time - min(Time))/(max(Time) - min(Time))
  McycleGrid <- makegrid(t=Time01, y=Accel)
  TimeGrid<-McycleGrid$gridt*(max(Time)-min(Time))+min(Time)
  McycleIRRWD <- irregwd(McycleGrid)
  McycleIRRWD2 <- McycleIRRWD
  class(McycleIRRWD2) <- "wd"
  McycleT1 <- threshold(McycleIRRWD, policy="universal",type=rule, dev=madmad)
  last(wr(McycleT1))
}
M6 <- sapply(c("soft", "hard"), function(x){ sum((SPX_test - 
                                            sapply(504:545, function(y) last(waveletown(SPX[1:y], rule = x))))^2)/41
})
plot(M6, type = "l", xaxt = "n")
axis(1, at = 1:2, labels = c("soft", "hard"))
```

## Trading strategy based on trend

We implement filters with optimal parameters that we've discussed so far to predict the next 5 year daily price. Once the filter responded a positive trend we will long the index while a negative trend will leads to shorting the index.

```{r results="hide", echo=FALSE, warning=FALSE, message=FALSE}
SP <- log(Quandl("YAHOO/INDEX_GSPC", type = "xts", start_date="2008-01-01", end_date ="2017-03-10")$Adj)

#strategy, when the trend is upward we buy otherwise we sell
real <- SP[2015:2313]
fore <- list()
fore$SMA <- ifelse(sapply(2015:2313, function(y) last(na.omit(rollapply(SP[1:y],30, FUN = mean, align = "right"))))>real, 1, -1)

l2pred <- sapply(2015:2313, function(y){
    ts <- l2filter.sparse(SP[1:y], lambda = 0.06)
    last(ts) - last(ts, 2)
  })
fore$L2 <- ifelse(l2pred>0, 1, -1)

kalpred <- sapply(2015:2313, function(y){
    tss <- kmfilter(SP[1:y], lambda = 0.12)
    coredata(last(tss)) * (1 - exp(-0.12)) - sum((1-exp(-0.12))*(exp(0.12)-1)*exp(-0.12 * 1:y) * coredata(SP[1:y]))
  })
fore$Kalman <- ifelse(kalpred > 0, 1, -1)

# modify a little about the loess function
loessown1 <- function(x, lambda = 0.0){
  da <- data.frame(cbind(coredata(x),1:length(x)))
  names(da) <- c("y", "x")
  vec <- loess(y~x, da, span = lambda)$fitted
  last(vec) - last(vec, 2)[1]
}

loepred <- sapply(2015:2313, function(y) last(loessown1(SP[1:y], lambda = 0.02)))
fore$Loess <- ifelse(loepred > 0, 1, -1)

# modify to get the slope from L1 result
l1fs1 <- function(x, lambda = 0.0){
  da <- coredata(x)
  fitted <- l1tf(da, lambda = lambda)
  last(fitted) - last(fitted, 2)[1]
}
l1pred <- sapply(2015:2313, function(y) last(l1fs1(SP[1:y], lambda = 0.001)))
fore$L1 <- ifelse(l1pred > 0, 1, -1)

# modify to get the slope as wavelet's trend prediction
waveletown1 <- function(x, rule = "soft"){
  Time <- 1:length(x)
  Accel <- coredata(x)
  Time01 <- (Time - min(Time))/(max(Time) - min(Time))
  McycleGrid <- makegrid(t=Time01, y=Accel)
  TimeGrid<-McycleGrid$gridt*(max(Time)-min(Time))+min(Time)
  McycleIRRWD <- irregwd(McycleGrid)
  McycleIRRWD2 <- McycleIRRWD
  class(McycleIRRWD2) <- "wd"
  McycleT1 <- threshold(McycleIRRWD, policy="universal",type=rule, dev=madmad)
  last(wr(McycleT1)) - last(wr(McycleT1), 2)[1]
}
wavpred <- sapply(2015:2313, function(y) last(waveletown1(SP[1:y], rule = "soft")))
fore$Wavelet <- ifelse(wavpred > 0, 1, -1)
```
```{r results="hide", echo=FALSE}
pnl <- list()

pnl$L2 <- na.omit(diff(SP[2015:2314]))*fore$L2
# plot(pnl$L2)
# sum(pnl$L2)

pnl$Kalman <- na.omit(diff(SP[2015:2314]))*fore$Kalman
# plot(pnl$Kalman)
# sum(pnl$Kalman)

pnl$Loess <- na.omit(diff(SP[2015:2314]))*fore$Loess
# plot(pnl$Loess)
# sum(pnl$Loess)

pnl$L1 <- na.omit(diff(SP[2015:2314]))*fore$L1
# plot(pnl$L1)
# sum(pnl$L1)

pnl$Wavelet <- na.omit(diff(SP[2015:2314]))*fore$Wavelet
# plot(pnl$Wavelet)
# sum(pnl$Wavelet)

# calculate the parameters of each trading results
hits <- c(L2 = sum(pnl$L2 > 0)/length(pnl$L2), Kalman = sum(pnl$Kalman > 0)/length(pnl$Kalman),
          Loess = sum(pnl$Loess > 0)/length(pnl$Loess), L1 = sum(pnl$L1 > 0)/length(pnl$L1),
          Wavelet = sum(pnl$Wavelet > 0)/length(pnl$Wavelet))
dayswin <- c(L2 = sum(pnl$L2 > 0), Kalman = sum(pnl$Kalman > 0),
          Loess = sum(pnl$Loess > 0), L1 = sum(pnl$L1 > 0),
          Wavelet = sum(pnl$Wavelet > 0))
dayslose <- c(L2 = sum(pnl$L2 < 0), Kalman = sum(pnl$Kalman < 0),
          Loess = sum(pnl$Loess < 0), L1 = sum(pnl$L1 < 0),
          Wavelet = sum(pnl$Wavelet < 0))
win <- c(L2 = mean(pnl$L2[pnl$L2 > 0]), Kalman = mean(pnl$Kalman[pnl$Kalman > 0]),
          Loess = mean(pnl$Loess[pnl$Loess > 0]), L1 = mean(pnl$L1[pnl$L1 > 0]),
          Wavelet = mean(pnl$Wavelet[pnl$Wavelet > 0]))
lose <- c(L2 = mean(pnl$L2[pnl$L2 < 0]), Kalman = mean(pnl$Kalman[pnl$Kalman < 0]),
          Loess = mean(pnl$Loess[pnl$Loess < 0]), L1 = mean(pnl$L1[pnl$L1 < 0]),
          Wavelet = mean(pnl$Wavelet[pnl$Wavelet < 0]))

rest <- cbind(hits, dayswin, dayslose, win, lose)

library(knitr)
```

```{r echo=FALSE}
kable(rest)
```

<!-- ## Trend detection -->

<!-- To apply filters to data we need to see if trend does exist. -->

<!-- We use Mann's (1945) statistics to detect trend: -->
<!-- $$\mathbb S_t^{(n)} = \sum_{i=0}^{n-2} \sum_{j=i+1}^{n-1} \text{sgn} (y_{t-i} - y_{t-j}) $$ -->
<!-- $$-\frac{n(n+1)}{2} \leq \mathbb S_t^{(n)} \leq \frac{n(n+1)}{2} $$ -->
<!-- If there is no trend, $\mathbb S_t^{(n)} \simeq 0 $, then we can test $Z_t^{(n)}$ -->
<!-- $$Z_t^{(n)} = \frac{\mathbb S_t^{(n)}}{\sqrt{\text{var} \left(\mathbb S_t^{(n)} \right)}} $$ -->

<!-- ## Trend detection, cont'd 1 -->
<!-- The paper compiled a table of significance to reject the $\mathbb S_t^{(n)} = 0$ hypothesis based on certain $\alpha$ value and window length. -->

<!-- ------------------------------------ -->
<!-- $\alpha$        90%    95%     99% -->
<!-- ------------- ------ ------- ------- -->
<!--   n=10 days   58.06% 49.47%  29.37% -->

<!--   n=3 months  85.77% 82.87%  76.68% -->

<!--   n=1 year    97.17% 96.78%  95.33% -->
<!-- ------------------------------------ -->

<!-- ## Prediction results -->

<!-- The paper studied returns across indxes for one-month return following a 3 month period of positive/negative trend. -->

<!-- ------------------------------------------- -->
<!-- Trend       Positive  Negative  Difference   -->
<!-- ----------- --------- --------- ----------- -->
<!-- Eurostoxx50    1.1%      0.2%      0.9% -->

<!-- S&P500         0.9%      0.5%      0.4% -->

<!-- MSCI WORLD     0.6%     -0.3%      1.0% -->

<!-- MSCI EM        1.9%     -0.3%      2.2% -->

<!-- TOPIX          0.4%     -0.4%      0.9% -->

<!-- EUR/USD        0.2%     -0.2%      0.4% -->

<!-- USD/JPY        0.2%     -0.2%      0.4% -->

<!-- GSCI           1.3%     -0.4%      1.6% -->

<!-- ------------------------------------------- -->

<!-- It clearly states the usefulness of trend strategies. Once we can obtain the trend using our strategies metioned above we can hopefully capture profits. -->

<!-- ## Trading based on moving average crossover -->


<!-- ```{r results="hide"} -->
<!-- knitr::opts_chunk$set(echo = FALSE, warnings=FALSE, error = FALSE, message = FALSE, results = "hide") -->
<!-- library(quantmod) -->
<!-- library(PerformanceAnalytics) -->
<!-- require(RCurl) -->
<!-- sit = getURLContent('https://github.com/systematicinvestor/SIT/raw/master/sit.gz', binary=TRUE, followlocation = TRUE, ssl.verifypeer = FALSE) -->
<!-- con = gzcon(rawConnection(sit, 'rb')) -->
<!-- source(con) -->
<!-- close(con) -->
<!-- data <- new.env() -->

<!-- # Load historical data and adjusts for splits and dividends -->
<!-- tickers = spl('SPY') -->
<!-- getSymbols(tickers, src = 'yahoo', from = '2000-01-01', env = data, auto.assign = T)     -->
<!-- for(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)   -->

<!-- #Calculate the moving averages and lag them one day to prevent lookback bias -->
<!-- PreviousSMA_50 <- lag(SMA(Cl(data[['SPY']]),50))   -->
<!-- PreviousSMA_200 <- lag(SMA(Cl(data[['SPY']]),200)) -->

<!-- #Sets backtesting environment -->
<!-- bt.prep(data, align='remove.na')  -->
<!-- prices = data$prices    -->

<!-- #Create a empty list for attaching the models to at a later stage -->
<!-- models = list() -->

<!-- #Specify the weights to be used in the backtest -->
<!-- data$weight[] = NA #Zero out any weights from previous -->
<!-- data$weight[] = ifelse(as.integer(PreviousSMA_50>PreviousSMA_200)==1,1,-1)  #If price of SPY is above the SMA then buy -->

<!-- #Call the function to run the backtest given the data, which contains the prices and weights. -->
<!-- models$technical_model = bt.run.share(data, trade.summary=T) -->

<!-- #Plot equity curve and export the trades list to csv -->
<!-- plot(models$technical_model$equity, main="Equity Curve") -->

<!-- # Returns -->
<!-- dailyBH<-models$technical_model$equity/na.omit(lag.xts(models$technical_model$equity))-1 -->

<!-- # Return Statistics  -->
<!-- annualizedReturn <- Return.annualized(dailyBH,scale=252, geometric=FALSE) -->
<!-- cumulativeReturn <-Return.cumulative(dailyBH) -->

<!-- # Risk Statistics -->
<!-- annualizedSTD <-sd.annualized(dailyBH,scale=252) -->
<!-- maxDrawdown <- maxDrawdown(dailyBH) -->

<!-- # Sharpe Ratio -->
<!-- excessBH <- dailyBH[2:length(dailyBH)]-.02/252 -->
<!-- dailyBHstd <- sd(dailyBH) -->
<!-- avgExcessBH <- mean(excessBH) -->

<!-- sharpeBH <- 10*(avgExcessBH/dailyBHstd*sqrt(252)) -->
<!-- ``` -->

## Refrences
[1] TREND FILTERING METHODS FOR MOMENTUM STRATEGIES, Benjamin Bruder et al, Lyxor Asset Management

[2] Understanding Extended Kalman Filter ¨C Part II: Multi-dimensional Kalman Filter, Shudong Huang, University of Technology Sydney

[3] Wavelet Methods in Statistics with R, Guy Nason
